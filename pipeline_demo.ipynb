{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0abddb5",
   "metadata": {},
   "source": [
    "# OBRAG Pipeline Demo\n",
    "\n",
    "Starting in a notebook to get pattern consistent before compositiing into single application.\n",
    "\n",
    "Starting with imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "371bcbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet --upgrade -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a3d61f",
   "metadata": {},
   "source": [
    "We'll start by setting up our chat model, embedding model, and vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49802534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat model\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13160f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saafetensors/OBRAG/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# embedding model\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model=\"Qwen/Qwen3-Embedding-0.6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6022b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector store\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vstore = Chroma(\n",
    "    collection_name=\"demo\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_demo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b691b413",
   "metadata": {},
   "source": [
    "Next, we can use some of LangChain's builtin Obsidian tools to gather and chunk our markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5481a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ObsidianLoader\n",
    "\n",
    "loader = ObsidianLoader(\n",
    "    path=\"/home/saafetensors/Documents/ollin/\",\n",
    "    collect_metadata=False\n",
    ")\n",
    "chunks = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bd6247",
   "metadata": {},
   "source": [
    "We can then add these chunks to the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34b0de95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 253/253 [00:15<00:00, 16.45it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for chunk in tqdm(chunks):\n",
    "    vstore.add_documents([chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5006950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saafetensors/OBRAG/.venv/lib/python3.10/site-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from typing_extensions import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve(state: State):\n",
    "    docs = vstore.similarity_search(state[\"question\"])\n",
    "    return {\"context\": docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eb944db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [Document(id='296fd158-b287-4bd3-87d5-5f50574cf7aa', metadata={'created': 1748955714.7666247, 'source': '21 POMDP Search Methods.md', 'path': '/home/saafetensors/Documents/ollin/archived/Notes/Lecture Notes/6.4110/21 POMDP Search Methods.md', 'last_accessed': 1750349477.3930354, 'last_modified': 1745948752.532999}, page_content=\"Generally, we want to choose actions to maximize expected discounted sum of rewards. Similar to last time, we'll capture the uncertainty by defining a *belief MDP*, we make a reduction from a POMDP to an MDP:\\n- states are *distributions* over $\\\\mathcal S$, existing on a *probability simplex* consisting of the vectors $(p_a, p_b, p_c)$ with $p_a + p_b + p_c = 1$ and $p_i \\\\in [0, 1]$; belief states are points on this simplex, so the state space is now *continuous*\\n- reward function looks like $$R(b, a) =  \\\\sum_s R(s, a) b(s)$$\\n\\t- simply just the expected reward based on the reward we already know in the original space, weighted by the state distribution defined by our belief\\n- transition function:$$T'(b, a, b') = P(b' | b, a)$$\\nTo do a belief update, we using something like a *bayes filter*:\\n$$\\nbf(b, a, o) \\\\to b'\\n$$\\na *deterministic* function that gives a new belief, $bf(b, a, o)$ is a distribution over states.\\n$$\\nbf(b, a, o)(s) \\\\propto P(o| a, s') \\\\sum_s P(s' | s, a)b(s)\\n$$\\nwith the bayes filter as a tool, we can now find our belief state transition probability for our transition function:\\n$$\\n\\\\displaylines{\\np(b' | b, a) = \\\\sum_{\\\\{o | b' = bf(b, a, o) \\\\}}p(o | b, a), \\\\\\\\\\np(o|b , a) = \\\\sum_s b(s) \\\\sum_{s'} T(s, a, s') O(s', a, o), \\\\\\\\\\nO(s', a, o) = p(o| s', a)\\n}\\n$$\\nthis gives you the expectation over all observations giving that belief, which is itself an expectation over possible belief states of an expected likelihood of gathering that observation. So overall, the likelihood of having that belief is the likelihood of getting one of the observations that give this belief given the previous context\\n\\n## Online Search\\nWe have already got all of the concepts that we need to have for this idea, can use an AND-OR search on the tree defined by actions and possible observations; computationally expensive, but can still do expectimax using some speed-up techniques like *sparse sampling* \\n\\nCan also employ *MCTS*, there is a special one *POMCP* that is specifically for these kinds of problems but has its own drawbacks (own thing to study, really).\"), Document(id='26a78529-81b0-41cf-9c20-a4e2c00840c4', metadata={'created': 1748955714.7666247, 'source': '21 POMDP Search Methods.md', 'path': '/home/saafetensors/Documents/ollin/archived/Notes/Lecture Notes/6.4110/21 POMDP Search Methods.md', 'last_modified': 1745948752.532999, 'last_accessed': 1750349477.3930354}, page_content=\"Generally, we want to choose actions to maximize expected discounted sum of rewards. Similar to last time, we'll capture the uncertainty by defining a *belief MDP*, we make a reduction from a POMDP to an MDP:\\n- states are *distributions* over $\\\\mathcal S$, existing on a *probability simplex* consisting of the vectors $(p_a, p_b, p_c)$ with $p_a + p_b + p_c = 1$ and $p_i \\\\in [0, 1]$; belief states are points on this simplex, so the state space is now *continuous*\\n- reward function looks like $$R(b, a) =  \\\\sum_s R(s, a) b(s)$$\\n\\t- simply just the expected reward based on the reward we already know in the original space, weighted by the state distribution defined by our belief\\n- transition function:$$T'(b, a, b') = P(b' | b, a)$$\\nTo do a belief update, we using something like a *bayes filter*:\\n$$\\nbf(b, a, o) \\\\to b'\\n$$\\na *deterministic* function that gives a new belief, $bf(b, a, o)$ is a distribution over states.\\n$$\\nbf(b, a, o)(s) \\\\propto P(o| a, s') \\\\sum_s P(s' | s, a)b(s)\\n$$\\nwith the bayes filter as a tool, we can now find our belief state transition probability for our transition function:\\n$$\\n\\\\displaylines{\\np(b' | b, a) = \\\\sum_{\\\\{o | b' = bf(b, a, o) \\\\}}p(o | b, a), \\\\\\\\\\np(o|b , a) = \\\\sum_s b(s) \\\\sum_{s'} T(s, a, s') O(s', a, o), \\\\\\\\\\nO(s', a, o) = p(o| s', a)\\n}\\n$$\\nthis gives you the expectation over all observations giving that belief, which is itself an expectation over possible belief states of an expected likelihood of gathering that observation. So overall, the likelihood of having that belief is the likelihood of getting one of the observations that give this belief given the previous context\\n\\n## Online Search\\nWe have already got all of the concepts that we need to have for this idea, can use an AND-OR search on the tree defined by actions and possible observations; computationally expensive, but can still do expectimax using some speed-up techniques like *sparse sampling* \\n\\nCan also employ *MCTS*, there is a special one *POMCP* that is specifically for these kinds of problems but has its own drawbacks (own thing to study, really).\"), Document(id='d1349596-6a17-48c8-8797-2b7ca6464154', metadata={'last_accessed': 1750349477.3920355, 'path': '/home/saafetensors/Documents/ollin/archived/Notes/Lecture Notes/6.4110/22 POMDP Dynamic Progamming.md', 'created': 1748955711.7836251, 'last_modified': 1746039568.825999, 'source': '22 POMDP Dynamic Progamming.md'}, page_content='Last time covered search in POMDPs, bayes filter. POMDP setting consistent with agent/obs/reward/policy cycle from earlier;\\n\\n`BayesUpdate` gives us the ability to filter observations into updated belief states. Choose policy same way from new belief state.\\n\\nIn the POMDP case in particular, our belief is a *distribution* over possible states, since our exact state is not observable. At a given time step, $t$, get observation $o_t$, belief=`BayesUpdate(b_{t-1}, o_t, a_{t-1})`  to give distribution over next possible states.\\n\\nLast class covered the first half of the cycle, getting new belief from observation; now how to build policy from new belief?\\n\\nWe want to use ideas we already know, so goal is to reduce it to something can use *value iteration* for.\\n\\n### how do we represent a policy?\\nHave previously represented policies as a **policy tree**: nodes are *actions*, edges are *observations* we get as a result. Take root action, make subsequent decisions based on given observation. With enough depth, becomes a policy *graph*, sort of small state machine over action-observation space.\\n\\nCan also literally represent it as a mapping from *belief* to *action*, like $$\\\\pi : B \\\\to A.$$\\nBut the notion of a policy tree assumes some *initial belief*; given no observations, we have a deterministic \"expected best action\" even if the underlying world state can be entirely different across trials.\\n\\nImplicitly, another way to develop a policy representation (which we\\'ve done before) is through defining a **value function**. Then, can just implicitly choose the actions at any given belief to maximize this value function:\\n$$\\nV^*: B \\\\to \\\\mathbb R\\n$$\\nWe have that $B$ is the space of all possible distributions over states in the POMDP; in the discrete state space case, this becomes a probability simplex over the states.\\n\\nWe\\'ll approach assigning a value by thinking of values for *policy trees*:\\n\\nGiven a policy tree $\\\\pi$, the value assigned is\\n$$\\nV^\\\\pi(s) = \\\\text{vlaue of executing $\\\\pi$ in state $s$}\\n$$\\nnot super explicit, so define it recursively:\\n\\nFor horizon = 1,\\n$$\\nV^\\\\pi (s) = R(s, \\\\pi(s))\\n$$\\nFor horizon $\\\\geq 2$:\\n$$\\nV^\\\\pi(s) = R(s, \\\\pi(s)) + \\\\gamma \\\\sum_s\\'  P(s\\' | s, \\\\pi(s)) \\\\sum_o P(o | \\\\pi(s), s\\') V^{\\\\pi|o}(s\\')\\n$$\\n\\nCan put all of this info into an *alpha vector*\\n$$\\n\\\\alpha_\\\\pi = [V^\\\\pi (s_1), \\\\ldots V^\\\\pi(s_n)]\\n$$\\n\\nThat way, our *belief state* value can be easily computed as\\n$$\\nV^\\\\pi(b) = b \\\\cdot \\\\alpha_\\\\pi\\n$$\\nAs a weighted sum of values according to $\\\\alpha_\\\\pi$.\\n\\nRest of class covers a tiger example for visualizing this policy behavior.'), Document(id='eabee394-8ebf-4976-a7c2-93a0a3c0cc1f', metadata={'path': '/home/saafetensors/Documents/ollin/archived/Notes/Lecture Notes/6.4110/22 POMDP Dynamic Progamming.md', 'source': '22 POMDP Dynamic Progamming.md', 'last_modified': 1746039568.825999, 'created': 1748955711.7836251, 'last_accessed': 1750349477.3920355}, page_content='Last time covered search in POMDPs, bayes filter. POMDP setting consistent with agent/obs/reward/policy cycle from earlier;\\n\\n`BayesUpdate` gives us the ability to filter observations into updated belief states. Choose policy same way from new belief state.\\n\\nIn the POMDP case in particular, our belief is a *distribution* over possible states, since our exact state is not observable. At a given time step, $t$, get observation $o_t$, belief=`BayesUpdate(b_{t-1}, o_t, a_{t-1})`  to give distribution over next possible states.\\n\\nLast class covered the first half of the cycle, getting new belief from observation; now how to build policy from new belief?\\n\\nWe want to use ideas we already know, so goal is to reduce it to something can use *value iteration* for.\\n\\n### how do we represent a policy?\\nHave previously represented policies as a **policy tree**: nodes are *actions*, edges are *observations* we get as a result. Take root action, make subsequent decisions based on given observation. With enough depth, becomes a policy *graph*, sort of small state machine over action-observation space.\\n\\nCan also literally represent it as a mapping from *belief* to *action*, like $$\\\\pi : B \\\\to A.$$\\nBut the notion of a policy tree assumes some *initial belief*; given no observations, we have a deterministic \"expected best action\" even if the underlying world state can be entirely different across trials.\\n\\nImplicitly, another way to develop a policy representation (which we\\'ve done before) is through defining a **value function**. Then, can just implicitly choose the actions at any given belief to maximize this value function:\\n$$\\nV^*: B \\\\to \\\\mathbb R\\n$$\\nWe have that $B$ is the space of all possible distributions over states in the POMDP; in the discrete state space case, this becomes a probability simplex over the states.\\n\\nWe\\'ll approach assigning a value by thinking of values for *policy trees*:\\n\\nGiven a policy tree $\\\\pi$, the value assigned is\\n$$\\nV^\\\\pi(s) = \\\\text{vlaue of executing $\\\\pi$ in state $s$}\\n$$\\nnot super explicit, so define it recursively:\\n\\nFor horizon = 1,\\n$$\\nV^\\\\pi (s) = R(s, \\\\pi(s))\\n$$\\nFor horizon $\\\\geq 2$:\\n$$\\nV^\\\\pi(s) = R(s, \\\\pi(s)) + \\\\gamma \\\\sum_s\\'  P(s\\' | s, \\\\pi(s)) \\\\sum_o P(o | \\\\pi(s), s\\') V^{\\\\pi|o}(s\\')\\n$$\\n\\nCan put all of this info into an *alpha vector*\\n$$\\n\\\\alpha_\\\\pi = [V^\\\\pi (s_1), \\\\ldots V^\\\\pi(s_n)]\\n$$\\n\\nThat way, our *belief state* value can be easily computed as\\n$$\\nV^\\\\pi(b) = b \\\\cdot \\\\alpha_\\\\pi\\n$$\\nAs a weighted sum of values according to $\\\\alpha_\\\\pi$.\\n\\nRest of class covers a tiger example for visualizing this policy behavior.')]\n",
      "Answer: In the POMDP setting, to use the Bayesian approach, we define a belief MDP where states are distributions over the original state space, and we utilize a Bayesian update mechanism via a Bayes filter to update our belief based on observations. This involves calculating the reward function based on the belief state and using transition functions that incorporate belief updates to capture the dynamics of the system (Context). We aim to maximize the expected discounted sum of rewards, adapting our policies as the beliefs change (Context).\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"What do we do when we are in the POMDP setting and want to use the Bayesian approach? Cite the documents you use.\"})\n",
    "\n",
    "print(f\"Context: {response['context']}\")\n",
    "print(f\"Answer: {response['answer']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
